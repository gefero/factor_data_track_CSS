---
title: "Clase 3. Relaciones entre palabras: n-gramas y  correlaciones"
subtitle: "Los diarios de Emilio Renzi (tres tomos)"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
author: "Germán Rosati"

date: "`r format(Sys.time(), '%d %B, %Y')`"
---

## Introducción
Hasta este punto, hemos considerado las palabras como unidades individuales y hemos examinado sus relaciones con los sentimientos o los documentos. Sin embargo, muchos análisis de texto interesantes se basan en las relaciones entre palabras, ya sea al examinar qué palabras tienden a seguir a otras inmediatamente, o qué palabras tienden a co-ocurrir dentro de los mismos documentos.

En este capítulo, exploraremos algunos de los métodos que ofrece `tidytext` para calcular y visualizar relaciones entre palabras en tu conjunto de datos de texto. Esto incluye el argumento `token = "ngrams"`, que realiza una tokenización de pares de palabras adyacentes en lugar de palabras individuales. También introduciremos dos paquetes nuevos: `ggraph`, que extiende `ggplot2` para construir gráficos de red, y `widyr`, que calcula correlaciones y distancias entre pares dentro de un marco de datos ordenado. Estas herramientas amplían nuestra caja de herramientas para explorar el texto dentro del marco de datos ordenado.

### Tokenización por n-grama
Hasta ahora, hemos estado utilizando la función `unnest_tokens` para tokenizar por palabra, o a veces por oración, lo cual es útil para los tipos de análisis de sentimiento y frecuencia que hemos estado haciendo hasta este punto. Pero también podemos utilizar esta función para tokenizar en secuencias consecutivas de palabras, lo que llamamos n-gramas. Al observar con qué frecuencia la palabra X es seguida por la palabra Y, podemos construir un modelo de sus relaciones.

Para lograr esto, agregamos la opción `token = "ngrams"` a la función `unnest_tokens()`, y establecemos el valor de `n` en el número de palabras que deseamos capturar en cada n-grama. Cuando establecemos `n` en 2, estamos examinando pares de dos palabras consecutivas, a menudo llamadas "bigramas".

```{r}
library(dplyr)
library(tidytext)
library(janeaustenr)

austen_bigrams <- austen_books() %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram))

austen_bigrams

```

Esta estructura de datos sigue siendo una variación del formato de texto ordenado. Está estructurada con un token por fila (con metadatos adicionales, como el libro, todavía conservados), pero cada token representa un bigrama.

Es importante notar que estos bigramas se superponen: "sentido y" es un token, mientras que "y sensibilidad" es otro.

## Conteo y filtrado de n-gramas
Nuestras herramientas habituales de tidy data se aplican igualmente bien al análisis de n-gramas. Podemos examinar los bigramas más comunes utilizando la función `count()` de dplyr:

```{r}
austen_bigrams %>%
  count(bigram, sort = TRUE)
```

Como uno podría esperar, muchas de las bigramas más comunes son pares de palabras comunes (poco interesantes),
como 'of the' (del) y 'to be' (ser o estar): lo que llamamos "stopwrod" (ver Capítulo 1). 
Este es un momento adecuado para usar la función `separate()` de `tidyr`, que divide una columna en varias 
basadas en un delimitador. Esto nos permite separarla en dos columnas, "palabra1" y "palabra2", momento en 
el cual podemos eliminar los casos en los que cualquiera de ellas sea una stopword.

```{r}
library(tidyr)

bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts
```

Podemos ver que los nombres (ya sea el nombre y apellido o con un saludo) son las parejas más comunes en los libros de Jane Austen.

En otros análisis, es posible que deseemos trabajar con las palabras recombinadas. La función `unite()` de `tidyr` es el inverso de `separate()`, y nos permite combinar las columnas en una sola. Así, "separate/filter/count/unite" nos permite encontrar las bigramas más comunes que no contienen stopwords.

```{r}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```


En otros análisis, es posible que estés interesado en los trigramas más comunes, que son secuencias consecutivas de 3 palabras. Podemos encontrar esto estableciendo `n = 3`:

```{r}
austen_books() %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  filter(!is.na(trigram)) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE)
```

## Analizando bigramas
Este formato de un bigrama por fila es útil para análisis exploratorios del texto. Como ejemplo sencillo, podríamos estar interesados en las calles más comunes mencionadas en cada libro:

```{r}
bigrams_filtered %>%
  filter(word2 == "street") %>%
  count(book, word1, sort = TRUE)
```

Un bigrama también puede ser tratado como un término en un documento de la misma manera que tratamos palabras individuales. Por ejemplo, podemos analizar el tf-idf de bigramas en las novelas de Austen. Estos valores de tf-idf pueden visualizarse dentro de cada libro, al igual que hicimos para las palabras.

```{r}
bigram_tf_idf <- bigrams_united %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf
```

Similar a lo que descubrimos las clases anteriores, las unidades que distinguen cada libro de Austen son casi exclusivamente nombres. También observamos algunas combinaciones de un verbo común y un nombre, como "replied elinor" en "Sensatez y Sentimientos", o "cried catherine" en "La Abadía de Northanger".

Existen ventajas y desventajas en examinar el tf-idf de bigramas en lugar de palabras individuales. Pares de palabras consecutivas pueden capturar estructuras que no están presentes cuando solo se cuentan palabras individuales, y pueden proporcionar contexto que hace que los tokens sean más comprensibles (por ejemplo, "maple grove" en "Emma" es más informativo que "maple"). Sin embargo, las cuentas por bigrama también son más dispersas: un par de palabras típico es más raro que cualquiera de sus palabras componentes. Por lo tanto, los bigramas pueden ser especialmente útiles cuando tienes un conjunto de datos de texto muy grande.

## Uso de bigramas para proporcionar contexto en el análisis de sentimiento
Nuestro enfoque de análisis de sentimiento que vimos la clase pasada simplemente contó la aparición de palabras positivas o negativas, de acuerdo con un léxico de referencia. Uno de los problemas con este enfoque es que el contexto de una palabra puede ser tan importante como su presencia. Por ejemplo, las palabras "feliz" y "gustar" serán contadas como positivas, incluso en una frase como "¡No estoy feliz y no me gusta!"

Ahora que tenemos los datos organizados en bigramas, es fácil determinar con qué frecuencia las palabras están precedidas por una palabra como "no":

```{r}
bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)
```

Realizando análisis de sentimiento en los datos de bigramas, podemos examinar con qué frecuencia las palabras asociadas al sentimiento son precedidas por "no" u otras palabras negadoras. Esto nos permitiría ignorar o incluso revertir su contribución al puntaje de sentimiento.

Utilizaremos el léxico AFINN para el análisis de sentimiento, que recordarás otorga un valor numérico de sentimiento para cada palabra, con números positivos o negativos indicando la dirección del sentimiento.

```{r}
AFINN <- get_sentiments("afinn")

AFINN
```

Luego podemos examinar las palabras más frecuentes que fueron precedidas por "no" y estaban asociadas con un sentimiento.


```{r}
not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE)

not_words
```

Por ejemplo, la palabra asociada con el sentimiento más común que sigue a "no" es "like" (gustar), que normalmente tendría una puntuación (positiva) de 2.

Vale la pena preguntarse qué palabras contribuyeron más en la dirección "equivocada". Para calcularlo, podemos multiplicar su valor por la cantidad de veces que aparecen (de modo que una palabra con un valor de +3 que ocurre 10 veces tenga el mismo impacto que una palabra con un valor de sentimiento de +1 que ocurra 30 veces). Visualizamos el resultado con un gráfico de barras (Figura 4.2).

```{r}
library(ggplot2)

not_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Sentiment value * number of occurrences",
       y = "Words preceded by \"not\"")
```

Los bigramas "not like" y "not help" fueron abrumadoramente las principales causas de mala identificación, haciendo que el texto pareciera mucho más positivo de lo que es. Sin embargo, podemos ver que frases como "not afraid" (no tener miedo) y "not fail" (no fracasar) a veces sugieren que el texto es más negativo de lo que realmente es.

"not" (no) no es el único término que proporciona algún contexto para la palabra siguiente. Podríamos elegir cuatro palabras comunes (o más) que nieguen el término subsiguiente, y utilizar el mismo enfoque de unión y conteo para examinar todas ellas a la vez.

```{r negation_analysis}
negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE)
```

Luego podríamos visualizar cuáles son las palabras más comunes que siguen a cada negación en particular. Si bien "not like" y "not help" siguen siendo los dos ejemplos más comunes, también podemos ver combinaciones como "no great" (no genial) y "never loved" (nunca amado). Podríamos combinar esto con los enfoques de la clase pasada para revertir los valores de AFINN de cada palabra que sigue a una negación. Estos son solo algunos ejemplos de cómo encontrar palabras consecutivas puede proporcionar contexto a los métodos de minería de texto.

## Visualización de una Red de Bigramas con `ggraph`
Podríamos estar interesados en visualizar todas las relaciones entre palabras de manera simultánea, en lugar de solo las principales algunas veces. Como una visualización común, podemos organizar las palabras en una red o "grafo". En este caso, nos referiremos a un "grafo" no en el sentido de una visualización, sino como una combinación de nodos conectados. Un grafo puede construirse a partir de un objeto tidy, ya que tiene tres variables:

*- from*: el nodo desde el cual parte un borde
*- to*: el nodo hacia el cual se dirige un borde
*- weight*: un valor numérico asociado con cada borde

El paquete `igraph` tiene muchas funciones poderosas para manipular y analizar redes. Una forma de crear un objeto igraph a partir de datos tidy es la función `graph_from_data_frame()`, que toma un data frame de bordes con columnas para "from", "to" y atributos de los bordes (en este caso, n):

```{r}
library(igraph)

# Recuentos originales
bigram_counts

# Filtrar solo combinaciones relativamente comunes
bigram_graph <- bigram_counts %>%
  filter(n > 20) %>%
  graph_from_data_frame()

bigram_graph
```

`igraph` tiene funciones de graficación incorporadas, pero muchos otros paquetes han desarrollado métodos de visualización para objetos de grafo. El paquete `ggraph` implementa estas visualizaciones en términos de la gramática de gráficos, con la que ya estamos familiarizados por `ggplot2`.

Podemos convertir un objeto igraph en un ggraph con la función `ggraph`, después de lo cual agregamos capas a él, de manera similar a cómo se agregan capas en `ggplot2`. Por ejemplo, para un gráfico básico necesitamos agregar tres capas: nodos, bordes y texto.

```{r}
library(ggraph)
set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

Más arriba, podemos visualizar algunos detalles de la estructura del texto. Por ejemplo, vemos que los saludos como "miss", "lady", "sir" y "colonel" forman centros comunes de nodos, que a menudo son seguidos por nombres. También vemos pares o tríos en el exterior que forman frases cortas comunes ("half hour", "thousand pounds" o "short time/pause").

Concluimos con algunas operaciones de pulido para hacer un gráfico de mejor apariencia:

- Agregamos el atributo edge_alpha a la capa de enlaces para hacer los enlaces transparentes según la frecuencia del bigrama (común o raro)

- Agregamos direccionalidad con una flecha, construida usando `grid::arrow()`, incluyendo la opción `end_cap` que indica que la flecha debe terminar antes de tocar el nodo

- Jugamos con las opciones de la capa de nodos para hacer los nodos más atractivos (puntos más grandes y azules)

- Agregamos un tema útil para trazar redes, `theme_void()`

```{r}
set.seed(2020)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

Puede requerir algo de experimentación con `ggraph` para lograr que las redes tengan un formato presentable como este, pero la estructura de red es una forma útil y flexible de visualizar datos relacionales tidy.

Es importante tener en cuenta que esta es una visualización de una cadena de Markov, un modelo común en el procesamiento de texto. En una cadena de Markov, cada elección de palabra depende únicamente de la palabra anterior. En este caso, un generador aleatorio que siga este modelo podría generar "dear" (querido), luego "sir" (señor), luego "william/walter/thomas/thomas’s", siguiendo cada palabra a las palabras más comunes que le siguen. Para hacer que la visualización sea interpretable, elegimos mostrar solo las conexiones más comunes de palabra a palabra, pero uno podría imaginar un grafo enorme que represente todas las conexiones que ocurren en el texto.

## Visualización de bigramas en otros textos
Hemos invertido un buen esfuerzo en limpiar y visualizar bigramas en un conjunto de datos de texto, así que recojamos esto en una función para que podamos realizarlo fácilmente en otros conjuntos de datos de texto.

Para que sea fácil usar las funciones `count_bigrams()` y `visualize_bigrams()` por ti mismo, también hemos vuelto a cargar los paquetes necesarios para ellas.

```{r}
library(dplyr)
library(tidyr)
library(tidytext)
library(ggplot2)
library(igraph)
library(ggraph)

count_bigrams <- function(dataset) {
  dataset %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>%
    count(word1, word2, sort = TRUE)
}

visualize_bigrams <- function(bigrams) {
  set.seed(2016)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}
```

En este punto, podríamos visualizar bigramas en otras obras, como la versión King James de la Biblia:

```{r}
# the King James version is book 10 on Project Gutenberg:
library(gutenbergr)
kjv <- gutenberg_download(10)
```

```{r}
library(stringr)

kjv_bigrams <- kjv %>%
  count_bigrams()

# filter out rare combinations, as well as digits
kjv_bigrams %>%
  filter(n > 40,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d")) %>%
  visualize_bigrams()
```

La anterior presenta un "plano" común del lenguaje dentro de la Biblia, especialmente centrado en "thy" (tu) y "thou" (tú) (¡que probablemente podrían considerarse stopwords!). Puedes utilizar el paquete gutenbergr y estas funciones count_bigrams/visualize_bigrams para visualizar bigramas en otros libros clásicos que te interesen.

## Conteo y correlación de pares de palabras con el paquete widyr
La tokenización por n-grama es una forma útil de explorar pares de palabras adyacentes. Sin embargo, también podríamos estar interesados en palabras que tienden a coocurrir dentro de documentos particulares o capítulos particulares, incluso si no ocurren una al lado de la otra.

Los datos tidy son una estructura útil para comparar entre variables o agrupar por filas, pero puede ser desafiante comparar entre filas: por ejemplo, contar la cantidad de veces que dos palabras aparecen en el mismo documento o ver cuán correlacionadas están. La mayoría de las operaciones para encontrar recuentos o correlaciones por pares necesitan convertir los datos en una matriz ancha primero.

Examinaremos algunas de las formas en que el texto tidy se puede convertir en una matriz ancha en el Capítulo 5, pero en este caso no es necesario. El paquete widyr facilita operaciones como calcular recuentos y correlaciones, al simplificar el patrón de "expandir datos, realizar una operación y luego reorganizar datos" (Figura 4.7). Nos centraremos en un conjunto de funciones que hacen comparaciones por pares entre grupos de observaciones (por ejemplo, entre documentos o secciones de texto).

![](../imgs/tmwr_0407.png)

## Conteo y Correlación entre Secciones
Considera el libro "Orgullo y Prejuicio" dividido en secciones de 10 líneas, como lo hicimos (con secciones más grandes) para el análisis de sentimiento en el Capítulo 2. Es posible que estemos interesados en las palabras que tienden a aparecer dentro de la misma sección.

```{r}
austen_section_words <- austen_books() %>%
  filter(book == "Pride & Prejudice") %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0) %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word)

austen_section_words
```

Una función útil de widyr es `pairwise_count()`. El prefijo `pairwise_` significa que esta función resultará en una fila por cada par de palabras en la variable "word". Esto nos permite contar pares comunes de palabras que coaparecen en la misma sección:

```{r}
library(widyr)

# count words co-occuring within sections
word_pairs <- austen_section_words %>%
  pairwise_count(word, section, sort = TRUE)

word_pairs
```

Es importante notar que mientras la entrada tenía una fila por cada par de un documento (una sección de 10 líneas) y una palabra, la salida tiene una fila por cada par de palabras. Esto también es un formato tidy, pero de una estructura muy diferente que podemos usar para responder nuevas preguntas.

Por ejemplo, podemos ver que el par de palabras más común en una sección es "Elizabeth" y "Darcy" (los dos personajes principales). También podemos encontrar fácilmente las palabras que más a menudo ocurren con "Darcy":

```{r}
word_pairs %>%
  filter(item1 == "darcy")
```

# Correlación por pares
Pares como "Elizabeth" y "Darcy" son las palabras que más comúnmente coocurren, pero eso no es particularmente significativo ya que también son las palabras individuales más comunes. En cambio, es posible que deseemos examinar la correlación entre palabras, lo que indica con qué frecuencia aparecen juntas en comparación con cuántas veces aparecen por separado.

En particular, aquí nos centraremos en el coeficiente phi, una medida común para la correlación binaria. El foco del coeficiente phi es cuánto más probable es que tanto la palabra X como la palabra Y aparezcan, o que ninguna aparezca, en comparación con que una aparezca sin la otra.

Considera la siguiente tabla:

|                       | Tiene la palabra Y | No iene la palabra Y | Total  |
|-----------------------|--------------------|----------------------|--------|
| Tiene la palabra X    | n_{11}             | n_{10}               | n_{1.} |
| No tiene la palabra X | n_{01}             | n_{00}               | n_{0.} |
| Total                 | n_{.1}             | n_{.0}               | n      |


Por ejemplo, que $n_{11}$, representa la cantidad de documentos en los que tanto la palabra X como la palabra Y aparecen, $n_{00}$ es el número en el que ninguna de las dos aparece, y
$n_{10}$ y $n_{01}$ son los casos en los que una aparece sin la otra. 

En términos de esta tabla, el coeficiente phi es:

$$\phi=\frac{n_{11}n_{00}-n_{10}n_{01}}{\sqrt{n_{1\cdot}n_{0\cdot}n_{\cdot0}n_{\cdot1}}}$$

El coeficiente phi es equivalente a la correlación de Pearson, que tal vez hayas escuchado en otros contextos cuando se aplica a datos binarios.

La función `pairwise_cor()` en widyr nos permite encontrar el coeficiente phi entre palabras basado en cuán frecuentemente aparecen juntas en la misma sección. Su sintaxis es similar a `pairwise_count()`.

```{r}
# we need to filter for at least relatively common words first
word_cors <- austen_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

word_cors
```

This output format is helpful for exploration. For example, we could find the words most correlated with a word like “pounds” using a filter operation.

Este ouput es útil para hacer un análisis exploratorio. Por ejemplo, podemos encontrar las palabras mas correlaciones con una palabra como "libras" usando una operación de filtrado:

```{r}
word_cors %>%
  filter(item1 == "pounds")
```

Esto nos permite elegir palabras específicas interesantes y encontrar las otras palabras más asociadas con ellas.

```{r}
word_cors %>%
  filter(item1 %in% c("elizabeth", "pounds", "married", "pride")) %>%
  group_by(item1) %>%
  slice_max(correlation, n = 6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()
```

Así como utilizamos ggraph para visualizar los bigrams, podemos usarlo para visualizar las correlaciones y agrupaciones de palabras que fueron encontradas por el paquete `widyr`:

```{r}
set.seed(2016)

word_cors %>%
  filter(correlation > .15) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

"Es importante destacar que, a diferencia del análisis de bigrams, las relaciones aquí son simétricas en lugar de direccionales (no hay flechas). También podemos observar que, aunque las combinaciones de nombres y títulos que dominaban las parejas de bigrams son comunes, como 'coronel/fitzwilliam', también podemos ver combinaciones de palabras que aparecen cerca una de la otra, como 'paseo' y 'parque', o 'baile' y 'fiesta'.

### Resumen
Este capítulo demostró cómo el enfoque de texto ordenado es útil no solo para analizar palabras individuales, sino también para explorar las relaciones y conexiones entre palabras. Tales relaciones pueden involucrar n-gramas, que nos permiten ver qué palabras tienden a aparecer después de otras, o coocurrencias y correlaciones, para palabras que aparecen en proximidad una de la otra. Este capítulo también demostró el paquete ggraph para visualizar ambos tipos de relaciones como redes. Estas visualizaciones de redes son una herramienta flexible para explorar relaciones y desempeñarán un papel importante en los estudios de caso en capítulos posteriores.