---
title: "Introducción a la regresión lineal múltiple (II)"
subtitle: "Multicolinealidad, selección de modelos y ejercicio integrador"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
author: "Germán Rosati"

date: "`r format(Sys.time(), '%d %B, %Y')`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE)
library(tidyverse)
library(broom)
```

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE, highlight=TRUE, paged.print=FALSE, prompt=TRUE, strip.white=FALSE, tidy = TRUE)
```

***
Este texto se basa en los siguientes materiales:

- Capítulo 7 del libro [Introduction to Modern Statistics](https://openintro-ims.netlify.app/index.html) de Mine Çetinkaya-Rundel y Johanna Hardin 
- Capítulos 5 del libro [Class Structure and Income Determination](https://drive.google.com/file/d/1_uzxqlkOmx_AG6T1uqILvNZIK9hIS4mN/view?usp=sharing) de Erik Olin Wright.
- Capitulo 3 del libro [Introduction to Statistical Learning](https://www.statlearning.com/) de Gareth James, Daniela Witten, Trevor Hastie y Rob Tibshirani

***

## Multicolinealidad

Pensemos brevemente en dos de las variables de nuestro modelo: `t_hogar` y `nivel_ed_agg`. ¿Hay correlación entre ambas?



 un problema común en la regresión múltiple: la correlación entre las variables predictoras. Decimos que las dos variables predictoras son colineales (pronunciadas como colineales) cuando están correlacionadas, y esta multicolinealidad complica la estimación del modelo. Si bien es imposible evitar que surja la multicolinealidad en los datos de observación, los experimentos generalmente se diseñan para evitar que los predictores sean multicolineales.

---

## Selección de modelos
El mejor modelo no siempre es el más complicado. A veces, incluir variables que no son evidentemente importantes puede reducir la precisión de las predicciones. En esta sección, analizamos las estrategias de selección de modelos, que nos ayudarán a eliminar del modelo las variables que se consideren menos importantes. Es común (y moderno, al menos en el mundo estadístico) referirse a modelos que han sufrido una poda tan variable como parsimoniosos.

En la práctica, el modelo que incluye todos los predictores disponibles suele denominarse modelo completo. El modelo completo puede no ser el mejor modelo, y si no lo es, queremos identificar un modelo más pequeño que sea preferible.

### Selección paso a paso (Stepwise selection)
Dos estrategias comunes para agregar o eliminar variables en un modelo de regresión múltiple se denominan eliminación hacia atrás y selección hacia adelante. Estas técnicas a menudo se conocen como estrategias de selección paso a paso, porque agregan o eliminan una variable a la vez a medida que avanzan a través de los predictores candidatos.

La eliminación hacia atrás (backwards) comienza con el modelo completo (el modelo que incluye todas las posibles variables predictoras). Las variables se eliminan una a la vez del modelo hasta que no podamos mejorarlo más.

La selección hacia adelante (forward) es lo contrario de la técnica de eliminación hacia atrás. En lugar de eliminar variables una a la vez, agregamos variables una a la vez hasta que no podamos encontrar ninguna variable que mejore más el modelo.

Una consideración importante al implementar cualquiera de estas estrategias de selección paso a paso es el criterio utilizado para decidir si eliminar o agregar una variable. Un criterio de decisión de uso común se ajusta R2. Cuando se usa ajustado R2 como criterio de decisión se busca eliminar o agregar variables dependiendo de si conducen a la mayor mejora en la R2 y nos detenemos cuando la adición o eliminación de otra variable no conduce a una mejora adicional en la R2.

El R2 ajustadodescribe la fuerza del ajuste de un modelo y es una herramienta útil para evaluar qué predictores agregan valor al modelo, donde agregar valor significa que (probablemente) mejoran la precisión en la predicción de resultados futuros.