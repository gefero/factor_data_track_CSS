---
title: "Aprendizaje Supervisado. Árboles de decisión"
subtitle: "Práctica independiente. CART de regresión"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
author: "Germán Rosati"

date: "`r format(Sys.time(), '%d %B, %Y')`"
---

La idea de esta práctica independiente es que utilicen las herramientas aprendidas para desarrollar un **árbol de decisión** en R con `tidymodels`.

Vamos a trabajar con datos del segundo trimestre de la EPH en 2015. Está guardada en la carpeta `data` con el nombre `eph15.RDS`.

```{r}
library(tidymodels)
library(tidyverse)

load('./data/EPH_2015_II.RData')

data <- data %>% 
           filter(imp_inglab1 != 1) # eliminamos los casos con no respuesta en ingresos.
```

Ahora, entramos a la parte de construcción del workflow de modelado. Recordemos que hay que:

-   Hacer la receta con la fórmula y, si los hay, los pasos de feature engineering que nos parezca que corresponde
    -   ¿Qué pasa con las variables de texto?
    -   ¿Está balanceada la muestra?
-   Partir el train/test
-   Instanciar el modelo, definiendo hiperparámetros
-   Probar con cross-validation cuál es la mejor combinación de hiperparámetros
-   Fitearlo a la base de entrenamiento

```{r}

data <- data %>% mutate(imp_inglab1 = as.factor(if_else(imp_inglab1==0, 'Con respuesta','Sin respuesta')))

```

¿Cómo funcionó este modelo en el test set? 

```{r}
set.seed(123)
eph_split <- initial_split(data)
eph_train <- training(eph_split)
eph_test <- testing(eph_split)
```

Y hacemos las muestras cross-validation:

```{r}
set.seed(234)
eph_cv <- vfold_cv(eph_train)
eph_cv
```

```{r}
recipe <- recipe(p21 ~ ., data = data) %>% step_rm("imp_inglab1")
```


```{r}
wf <- workflow() %>%
  add_recipe(recipe)
```


Ahora, armarmos la estructura del modelo pero con parámetros que queremos tunear. Por eso ponemos la función `tune()` en la definición de los distintos parámetros: el umbral de la métrica de pureza para definir la complejidad del árbol, la profundidad del árbol y el mínimo de variables que tiene que tener cada partición del nodo.

```{r}
tree_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
) %>%
  set_engine("rpart") %>%
  set_mode("regression")

tree_spec
```

```{r}
tree_wf <- wf %>%
  add_model(tree_spec)

tree_wf
```


Con la función `grid_regular()` armamos una serie de combinaciones aleatorias posibles para los parámetros del modelo. Le decimos que nos de 4 niveles de valores posibles para cada uno de los parámetros, y que los combine.

```{r}
tree_grid <- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 4)

tree_grid
```

Ahora vamos a probar los posibles valores que puede adoptar el modelo con los distintos parámetros en las muestras con cross-validation.


```{r}
library(future)
plan(multisession, workers = 8)

tree_rs <- tree_wf %>% 
  tune_grid(
  resamples = eph_cv,
  grid = tree_grid,
  metrics = metric_set(rmse, rsq, ccc)
)

tree_rs

```

```{r}
collect_metrics(tree_rs)
```

Con la función `autoplot()` podemos hacer de manera sencilla un gráfico que nos visualice las métricas de cada una de las variantes del modelo.

```{r}
autoplot(tree_rs) + theme_light(base_family = "IBMPlexSans")
```

Parecería que este dataset funciona mejor con un árbol no tan complejo. Podemos seguir examinando el mejor set de parámetros según la métrica que queramos.

```{r}
show_best(tree_rs, metric="rmse")
```

E incluso podemos elegir el mejor modelo de esas pruebas cross-validation para implementar en el modelo final.

```{r}
best_model <- select_best(tree_rs, metric="rmse")

final_tree <- finalize_model(tree_spec, best_model)

final_tree
```

Hasta acá, el modelo está actualizado y finalizado (no lo podemos seguir tuneando con distintos parámetros). Pero nos resta *fitearlo* al dataset de entrenamiento, lo que vamos a hacer con la función `fit()`.

```{r}
final_fit <- fit(final_tree, p21 ~ ., eph_train)


```

Este print nos muestra un bloque de texto con los nodos y distintas ramas. Sin embargo, también podemos visualizar las variables más importantes del modelo con el paquete `vip`. Con su función podemos mostrar de forma sencilla la importancia de las variables del modelo en un gráfico de columnas, puntos, boxplot o violin plot.

```{r}
library(vip)

final_fit %>%
  vip(geom = "col")
```

Podemos ver que las variables más importantes para explicar si un caso está imputado o no son `aglomerado` y `region`.

Ahora bien, el último paso sería probar esto en el set de validación o test set.

```{r}
eph_test <- final_fit %>%
  predict(eph_test) %>%
  bind_cols(eph_test, .)
```


```{r}
class_metrics <- metric_set(rmse, rsq, ccc)

# The returned function has arguments:
# fn(data, truth, estimate, na_rm = TRUE, ...)
class_metrics(eph_test, truth = p21, estimate = .pred)
```
